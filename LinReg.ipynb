{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e312d34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   2,   3,   4,   5,   1,   2,   3,   4,   5,   4,   6,   8,\n",
       "         10,   9,  12,  15,  16,  20,  25],\n",
       "       [  5,   6,   7,   8,   9,  25,  30,  35,  40,  45,  36,  42,  48,\n",
       "         54,  49,  56,  63,  64,  72,  81],\n",
       "       [  7,   8,   9,  10,  11,  49,  56,  63,  70,  77,  64,  72,  80,\n",
       "         88,  81,  90,  99, 100, 110, 121],\n",
       "       [ 11,  12,  13,  14,   5, 121, 132, 143, 154,  55, 144, 156, 168,\n",
       "         60, 169, 182,  65, 196,  70,  25]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PART-A\n",
    "#1. Generate polynomial and interaction features for a given degree of the polynomial\n",
    "import itertools\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "def polynomialFeatures(X, degree):   \n",
    "    X_poly=list()\n",
    "    #index=0\n",
    "    for row in X:\n",
    "        #index+=1 \n",
    "        #print(index)\n",
    "        for i in range(1,degree+1):\n",
    "            \n",
    "            for j in itertools.combinations_with_replacement(row, i):\n",
    "            \n",
    "                X_poly.append(np.product(j))\n",
    "                \n",
    "    X_poly=np.array(X_poly).reshape(X.shape[0],-1)\n",
    "    return X_poly\n",
    "\n",
    "\n",
    "polynomialFeatures(np.array([[1, 2, 3,4,5],[5,6,7,8,9],[7,8,9,10,11],[11,12,13,14,5]]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f35cedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 MSE\n",
    "\n",
    "def MSE(Y_true,Y_pred):\n",
    "    return (Y_true-Y_pred).T.dot(Y_true-Y_pred)/Y_true.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e06758c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 plot polynomial model complexity\n",
    "def plot_polynomial_model_complexity(model, X, Y, cv, maxPolynomialDegree=5, learning_rate=None,\n",
    "                                  epochs=None, tol=None, regularizer=None, lambd=None, **kwargs):\n",
    "    validrmselist=list()\n",
    "    trainrmselist=list()\n",
    "    degrees=list()\n",
    "    for degree in range(1,maxPolynomialDegree+1):\n",
    "        degrees.append(degree)\n",
    "        X_train_poly=polynomialFeatures(X_train,degree)\n",
    "        X_test_poly=np.zeros((X_train_poly.shape[0],X_train_poly.shape[1]))\n",
    "        X_train_polyfit, X_test_polyfit=standardize_data(X_train_poly,X_test_poly)\n",
    "\n",
    "        LR=linearRegression(learningrate=learning_rate, epochs=epochs, tol=tol, regularizer=\"L2\", lambd=lambd)\n",
    "        LR.fit(X_train_polyfit,y_train)\n",
    "        y_predict= LR.predict(X_train_polyfit)\n",
    "        mse=MSE(y_train,y_predict)\n",
    "        rmse=np.sqrt(mse)\n",
    "        trainrmselist.append(rmse)\n",
    "        \n",
    "        linreg=linearRegression(learningrate=learning_rate, epochs=epochs, tol=tol, regularizer=\"L2\", lambd=lambd)\n",
    "        pred, true, trainrmse, validrmse =kFold(cv,X_train_polyfit,y_train,linreg,\"rmse\")\n",
    "        validrmselist.append(validrmse)\n",
    "        \n",
    "\n",
    "    print(validrmselist)\n",
    "    print(trainrmselist)\n",
    "    #print(degrees)\n",
    "    Validmse = plt.plot(degrees,validrmselist, label='Valiation Data RMSE')\n",
    "    Trainmse = plt.plot(degrees,trainrmselist, label='Training Data RMSE')\n",
    "    plt.legend()\n",
    "    plt.title(\"Polynomial Model Complexity\")\n",
    "    plt.xlabel(\"Degree\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.show()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "17051d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Linear Regression model\n",
    "class linearRegression():\n",
    "    def __init__(self,learningrate=None, epochs=None,tol=None,regularizer=None,lambd=None,**kwargs):\n",
    "        self.learningrate=learningrate\n",
    "        self.epochs=epochs\n",
    "        self.tol=tol\n",
    "        self.regularizer=regularizer\n",
    "        self.lambd=lambd\n",
    "        self.kwargs=kwargs\n",
    "        \n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        m,d=X.shape\n",
    "        theta_hat=np.zeros(d)\n",
    "        newJ=0\n",
    "        if self.regularizer==\"L1\":\n",
    "                \n",
    "            for i in range(self.epochs):\n",
    "                theta_hat_regularized=theta_hat.copy()\n",
    "                theta_hat_regularized[0]=0\n",
    "                theta_hat=theta_hat-(self.learningrate/m)*X.T.dot(X.dot(theta_hat)-y)-((self.learningrate*self.lambd)/m)*(np.sign(theta_hat_regularized))\n",
    "                previousJ=newJ\n",
    "                newJ=(1/(2*m))*(y-X.dot(theta_hat)).T.dot(y-X.dot(theta_hat))+(self.lambd/(2*m))*np.sum(np.abs(theta_hat_regularized))\n",
    "                if abs(newJ-previousJ) < self.tol:\n",
    "                    break\n",
    "\n",
    "        elif self.regularizer==\"L2\":\n",
    "                \n",
    "            for i in range(self.epochs):\n",
    "                theta_hat_regularized=theta_hat.copy()\n",
    "                theta_hat_regularized[0]=0\n",
    "                theta_hat=theta_hat-(self.learningrate/m)*np.dot(X.T,np.dot(X,theta_hat)-y)-(self.learningrate*self.lambd/m)*(theta_hat_regularized)\n",
    "                previousJ=newJ\n",
    "                newJ=(1/(2*m))*(y-X.dot(theta_hat)).T.dot(y-X.dot(theta_hat))+(self.lambd/(2*m))*np.sum((theta_hat_regularized)**2)\n",
    "                if abs(newJ-previousJ) < self.tol:\n",
    "                    break\n",
    "\n",
    "        elif self.regularizer==\"none\":\n",
    "\n",
    "            for i in range(self.epochs):\n",
    "                theta_hat=theta_hat-(self.learningrate/m)*np.dot(X.T,np.dot(X,theta_hat)-y)\n",
    "                previousJ=newJ;\n",
    "                newJ=(1/(2*m))*(y-X.dot(theta_hat)).T.dot(y-X.dot(theta_hat))\n",
    "        \n",
    "        self.theta=theta_hat        \n",
    "        \n",
    "    def predict(self,X):\n",
    "        Y=X.dot(self.theta)\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c1dc7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PART-B\n",
    "#5 read in data\n",
    "import pandas as pd\n",
    "df=pd.read_csv(r'C:\\Users\\Sanjay Alamuru\\Downloads\\winequality-red.csv',sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4146bd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>5.636023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
       "mean      0.087467            15.874922             46.467792     0.996747   \n",
       "std       0.047065            10.460157             32.895324     0.001887   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             22.000000     0.995600   \n",
       "50%       0.079000            14.000000             38.000000     0.996750   \n",
       "75%       0.090000            21.000000             62.000000     0.997835   \n",
       "max       0.611000            72.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
       "mean      3.311113     0.658149    10.422983     5.636023  \n",
       "std       0.154386     0.169507     1.065668     0.807569  \n",
       "min       2.740000     0.330000     8.400000     3.000000  \n",
       "25%       3.210000     0.550000     9.500000     5.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6 Use the techniques from the recitation to summarize each of the variables in the dataset in \n",
    "#terms of mean, standard deviation, and quartiles. Include this in your report.\n",
    "\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3921c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 9.6  ,  0.77 ,  0.12 , ...,  3.3  ,  0.64 , 10.4  ],\n",
       "        [ 9.8  ,  0.66 ,  0.39 , ...,  3.37 ,  0.71 , 11.5  ],\n",
       "        [ 9.3  ,  0.61 ,  0.26 , ...,  3.24 ,  0.62 ,  9.7  ],\n",
       "        ...,\n",
       "        [ 6.3  ,  0.51 ,  0.13 , ...,  3.42 ,  0.75 , 11.   ],\n",
       "        [ 5.9  ,  0.645,  0.12 , ...,  3.57 ,  0.71 , 10.2  ],\n",
       "        [ 6.   ,  0.31 ,  0.47 , ...,  3.39 ,  0.66 , 11.   ]]),\n",
       " array([6, 7, 5, ..., 6, 5, 6], dtype=int64))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7. Shuffle the rows of your data. You can use def = df.sample(frac=1) \n",
    "#as an idiomatic way to shuffle the data in Pandas without losing column names.\n",
    "#Create a test dataset by randomly sampling 20% of the data. Remaining data should be used for training.\n",
    "import numpy as np\n",
    "df.sample(frac=1)\n",
    "\n",
    "X = np.array(df.drop( \"quality\", axis = 1))\n",
    "y = np.array(df['quality'])\n",
    "\n",
    "import math\n",
    "\n",
    "def partition(X, y, t):\n",
    "    part = math.trunc(t * len(y))\n",
    "    y_test = y[0:part]\n",
    "    y_train = y[part:len(y)]\n",
    "    X_test = X[0:part]\n",
    "    X_train = X[part:len(y)]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "X_train, X_test, y_train, y_test = partition(X,y,0.2)\n",
    "\n",
    "X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "02265c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART-C\n",
    "#8 Standardizing the data\n",
    "X_train_poly=polynomialFeatures(X_train,1)\n",
    "X_test_poly=polynomialFeatures(X_test,1)\n",
    "def standardize_data(x_train,x_test):\n",
    "    for i in range(x_train.shape[1]):\n",
    "        mean=np.mean(x_train[:,i])\n",
    "        std=np.std(x_train[:,i])\n",
    "        x_train[:,i]=(x_train[:,i]-mean)/std\n",
    "        x_test[:,i]=(x_test[:,i]-mean)/std\n",
    "    x_train=np.insert(x_train,0,1.0,axis=1)\n",
    "    x_test=np.insert(x_test,0,1.0,axis=1)\n",
    "    return x_train, x_test\n",
    "\n",
    "#print(standardize_data(X_train_poly,X_test_poly))\n",
    "X_train_poly,X_test_poly=standardize_data(X_train_poly,X_test_poly)\n",
    "#print(X_train_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "c5fb3b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+---------------+----------+\n",
      "|   lambd |   learningrate | regularizer   |      mse |\n",
      "+=========+================+===============+==========+\n",
      "|  1      |         0.1    | L1            | 0.437952 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  1      |         0.1    | L2            | 0.438452 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  1      |         0.01   | L1            | 0.439262 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  1      |         0.01   | L2            | 0.439458 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  1      |         0.001  | L1            | 0.445977 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  1      |         0.001  | L2            | 0.446188 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  1      |         0.0001 | L1            | 6.88483  |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  1      |         0.0001 | L2            | 6.88903  |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.1    |         0.1    | L1            | 0.438438 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.1    |         0.1    | L2            | 0.43849  |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.1    |         0.01   | L1            | 0.439455 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.1    |         0.01   | L2            | 0.439482 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.1    |         0.001  | L1            | 0.446197 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.1    |         0.001  | L2            | 0.446223 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.1    |         0.0001 | L1            | 6.88947  |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.1    |         0.0001 | L2            | 6.88989  |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.01   |         0.1    | L1            | 0.438489 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.01   |         0.1    | L2            | 0.438493 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.01   |         0.01   | L1            | 0.439481 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.01   |         0.01   | L2            | 0.439484 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.01   |         0.001  | L1            | 0.446219 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.01   |         0.001  | L2            | 0.446222 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.01   |         0.0001 | L1            | 6.88994  |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.01   |         0.0001 | L2            | 6.88998  |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.001  |         0.1    | L1            | 0.438493 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.001  |         0.1    | L2            | 0.438493 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.001  |         0.01   | L1            | 0.439484 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.001  |         0.01   | L2            | 0.439484 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.001  |         0.001  | L1            | 0.446222 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.001  |         0.001  | L2            | 0.446222 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.001  |         0.0001 | L1            | 6.88998  |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.001  |         0.0001 | L2            | 6.88999  |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.0001 |         0.1    | L1            | 0.438493 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.0001 |         0.1    | L2            | 0.438493 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.0001 |         0.01   | L1            | 0.439484 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.0001 |         0.01   | L2            | 0.439484 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.0001 |         0.001  | L1            | 0.446222 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.0001 |         0.001  | L2            | 0.446222 |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.0001 |         0.0001 | L1            | 6.88999  |\n",
      "+---------+----------------+---------------+----------+\n",
      "|  0.0001 |         0.0001 | L2            | 6.88999  |\n",
      "+---------+----------------+---------------+----------+\n"
     ]
    }
   ],
   "source": [
    "#8. cross-fold validation function\n",
    "import tabulate\n",
    "def partition_data(data,folds):\n",
    "    \n",
    "    split_idx=int(data.shape[0]/folds)\n",
    "    idx=[]\n",
    "    indexes= np.arange(0,data.shape[0])\n",
    "    \n",
    "    for f in range(folds):\n",
    "        \n",
    "        test = indexes[:split_idx]\n",
    "        train = indexes[split_idx:]\n",
    "        i=np.arange(0,split_idx)\n",
    "        indexes=np.delete(indexes,i)        \n",
    "        indexes=np.append(indexes,test)\n",
    "        idx.append((train,test))\n",
    "        \n",
    "    return np.asarray(idx)\n",
    "\n",
    "def kFold(folds, X, y, model, error_function,**model_args):\n",
    "    split_folds=partition_data(X,folds)\n",
    "    test_error = []\n",
    "    true_label = []\n",
    "    pred_label = []\n",
    "    train_error = []\n",
    "    \n",
    "    for train_index, test_index in split_folds:\n",
    "        X_t, X_te = X[train_index], X[test_index]\n",
    "        y_t, y_te = y[train_index], y[test_index]\n",
    "        \n",
    "        learning_rate_init, epoch_init , tol_init, regularizer_init, lambd_init  = 0.001, 1000, None,None,0.0 \n",
    "        \n",
    "        \n",
    "        for key, value in model_args.items(): \n",
    "            \n",
    "            if key=='learningrate':\n",
    "                learning_rate_init=value\n",
    "            if key=='epochs':\n",
    "                epoch_init=value\n",
    "            if key=='tol':\n",
    "                tol_init=value\n",
    "            if key==\"regularizer\":\n",
    "                regularizer_init=value\n",
    "            if key==\"lambd\":\n",
    "                lambd_init=value\n",
    "                \n",
    "            if key == \"degree\":\n",
    "                degree_init = value\n",
    "            \n",
    "        \n",
    "        model.fit(X_t, y_t)#, learningrate = learning_rate_init, epochs= epoch_init, tol = tol_init,\n",
    "                  #regularizer = regularizer_init,lambd = lambd_init)\n",
    "        \n",
    "        \n",
    "        \n",
    "        y_test_pred=model.predict( X_te)\n",
    "        y_train_pred = model.predict( X_t)\n",
    "            \n",
    "     \n",
    "        \n",
    "\n",
    "        true_label.append(y_te)\n",
    "        pred_label.append(y_test_pred)\n",
    "        \n",
    "        if error_function == \"mse\":\n",
    "            \n",
    "            avg_test_error = MSE(y_test_pred, y_te)\n",
    "            avg_train_error = MSE(y_train_pred, y_t)\n",
    "            \n",
    "        elif error_function == \"rmse\":\n",
    "            avg_test_error = np.sqrt(MSE(y_test_pred, y_te))\n",
    "            avg_train_error = np.sqrt(MSE(y_train_pred, y_t))\n",
    "        \n",
    "        test_error.append(avg_test_error)\n",
    "        train_error.append(avg_train_error)\n",
    "    \n",
    "    \n",
    "    return np.array(pred_label), np.array(true_label), np.mean(np.array(train_error)), np.mean(np.array(test_error))\n",
    "\n",
    "\n",
    "\n",
    "def mydictionary(lamb, learnrate, regular):\n",
    "    mydict={'lamb':[],'learningrate':[],'regularizer':[],'mse':[]}\n",
    "    mylist=[]\n",
    "    for l in lamb:\n",
    "        for lr in learnrate:\n",
    "            for r in regular:\n",
    "                linreg=linearRegression(learningrate=lr,epochs=10000,tol=1e-6,regularizer=r,lambd=l)\n",
    "                pred, true, trainmse, validmse =kFold(5,X_train_poly,y_train,linreg,\"mse\")\n",
    "                mydict={'lambd':l,'learningrate':lr,'regularizer':r,'mse':validmse}\n",
    "                mydict_copy=mydict.copy()\n",
    "                mylist.append(mydict_copy)\n",
    "    header=mylist[0].keys()\n",
    "    rows= [x.values() for x in mylist]\n",
    "    print(tabulate.tabulate(rows,header,tablefmt='grid'))\n",
    "lamb=[1.0,0.1,0.01, 0.001, 0.0001]\n",
    "learnrate= [0.1, 0.01, 0.001, 0.0001]\n",
    "regular=[\"L1\",\"L2\"]\n",
    "mydictionary(lamb,learnrate,regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65e522",
   "metadata": {},
   "source": [
    "with lambda of 1 and learning rate 0.1 with L1 regularization we get the best mse at 0.437952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "ec0fd098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4436356676056648, 0.44331808342582113)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#9Evaluate your model on the test data and report the mean squared error.\n",
    "\n",
    "#Batch GD on test data with L1(Lasso)\n",
    "linreg=linearRegression(learningrate=0.1,epochs=10000,tol=1e-6,regularizer=\"L1\",lambd=1.0)\n",
    "linreg.fit(X_train_poly,y_train)\n",
    "y_predl1=linreg.predict(X_test_poly)\n",
    "testmsel1=MSE(y_test,y_predl1)\n",
    "testrmsel1=np.sqrt(testmsel1)\n",
    "\n",
    "#Batch GD on test data with L2(Ridge)\n",
    "linreg=linearRegression(learningrate=0.1,epochs=10000,tol=1e-6,regularizer=\"L2\",lambd=1.0)\n",
    "linreg.fit(X_train_poly,y_train)\n",
    "y_predl2=linreg.predict(X_test_poly)\n",
    "testmsel2=MSE(y_test,y_predl2)\n",
    "testrmsel2=np.sqrt(testmsel2)\n",
    "\n",
    "\n",
    "testmsel1, testmsel2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70f97c",
   "metadata": {},
   "source": [
    "We used L1 regularization with lambda=1 and learning rate 0.1 to get a test mse of 0.443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "09067d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-216-9b626dfccb00>:24: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if abs(newJ-previousJ) < self.tol:\n",
      "<ipython-input-216-9b626dfccb00>:35: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if abs(newJ-previousJ) < self.tol:\n",
      "<ipython-input-216-9b626dfccb00>:34: RuntimeWarning: overflow encountered in square\n",
      "  newJ=(1/(2*m))*(y-X.dot(theta_hat)).T.dot(y-X.dot(theta_hat))+(self.lambd/(2*m))*np.sum((theta_hat_regularized)**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+---------------+------------+\n",
      "|   lambd |   learningrate | regularizer   |        mse |\n",
      "+=========+================+===============+============+\n",
      "|  1      |          0.1   | L1            | nan        |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  1      |          0.1   | L2            | nan        |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  1      |          0.01  | L1            |   0.500965 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  1      |          0.01  | L2            |   0.522798 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  1      |          0.001 | L1            |   0.782414 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  1      |          0.001 | L2            |   0.843326 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  1      |          0.001 | L1            |   0.782414 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  1      |          0.001 | L2            |   0.843326 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.1    |          0.1   | L1            | nan        |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.1    |          0.1   | L2            | nan        |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.1    |          0.01  | L1            |   0.519434 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.1    |          0.01  | L2            |   0.524919 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.1    |          0.001 | L1            |   0.840538 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.1    |          0.001 | L2            |   0.846861 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.1    |          0.001 | L1            |   0.840538 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.1    |          0.001 | L2            |   0.846861 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.01   |          0.1   | L1            | nan        |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.01   |          0.1   | L2            | nan        |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.01   |          0.01  | L1            |   0.524548 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.01   |          0.01  | L2            |   0.525187 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.01   |          0.001 | L1            |   0.84657  |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.01   |          0.001 | L2            |   0.84723  |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.01   |          0.001 | L1            |   0.84657  |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.01   |          0.001 | L2            |   0.84723  |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.001  |          0.1   | L1            | nan        |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.001  |          0.1   | L2            | nan        |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.001  |          0.01  | L1            |   0.525147 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.001  |          0.01  | L2            |   0.525213 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.001  |          0.001 | L1            |   0.847188 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.001  |          0.001 | L2            |   0.84724  |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.001  |          0.001 | L1            |   0.847188 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.001  |          0.001 | L2            |   0.84724  |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.0001 |          0.1   | L1            | nan        |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.0001 |          0.1   | L2            | nan        |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.0001 |          0.01  | L1            |   0.525209 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.0001 |          0.01  | L2            |   0.525213 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.0001 |          0.001 | L1            |   0.84724  |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.0001 |          0.001 | L2            |   0.847245 |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.0001 |          0.001 | L1            |   0.84724  |\n",
      "+---------+----------------+---------------+------------+\n",
      "|  0.0001 |          0.001 | L2            |   0.847245 |\n",
      "+---------+----------------+---------------+------------+\n",
      "Best Validation MSE is [0.5009645445553348]\n"
     ]
    }
   ],
   "source": [
    "#10 Determinethe best model hyperparameter values for the training data matrix with polynomial degree 3.\n",
    "import tabulate\n",
    "lamb=[1.0,0.1,0.01, 0.001, 0.0001]\n",
    "learnrate= [0.1, 0.01, 0.001, 0.001]\n",
    "regular=[\"L1\",\"L2\"]\n",
    "X_train_poly3=polynomialFeatures(X_train,3)\n",
    "X_test_poly3=polynomialFeatures(X_test,3)\n",
    "X_train_poly3,X_test_poly3=standardize_data(X_train_poly3,X_test_poly3)\n",
    "mydict={'lamb':[],'learningrate':[],'regularizer':[],'mse':[]}\n",
    "mylist=[]\n",
    "for l in lamb:\n",
    "    for lr in learnrate:\n",
    "        for r in regular:\n",
    "            linreg=linearRegression(learningrate=lr,epochs=10000,tol=1e-6,regularizer=r,lambd=l)\n",
    "            pred, true, trainmse, validmse =kFold(5,X_train_poly3,y_train,linreg,\"mse\")\n",
    "            mydict={'lambd':l,'learningrate':lr,'regularizer':r,'mse':validmse}\n",
    "            mydict_copy=mydict.copy()\n",
    "            mylist.append(mydict_copy)\n",
    "\n",
    "validmselist=list()\n",
    "bestvalidmse=list()\n",
    "for i in range(len(mylist)):\n",
    "    validmselist.append(mylist[i]['mse'])\n",
    "bestvalidmse.append(np.nanmin(validmselist))\n",
    "header=mylist[0].keys()\n",
    "rows= [x.values() for x in mylist]\n",
    "print(tabulate.tabulate(rows,header,tablefmt='grid'))\n",
    "print(\"Best Validation MSE is\", bestvalidmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cf0805",
   "metadata": {},
   "source": [
    "Best Hyperparameters with MSE 0.50096454455533348 are Lambda=1.0,Learning rate=0.01 and Regularizer=L1(Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "bb69a2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6672805699580489, 0.6820399390797712, 0.8787807606799456, 1.9380505076373915, 7.0983765665249]\n",
      "[0.6483550615154801, 0.6314778245780721, 0.6139848904608785, 0.5975411680110693, 0.5749951493216601]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#11 Plot polynomial model complexity for polynomial degrees 1, 2, 3, 4, 5\n",
    "\n",
    "plot_polynomial_model_complexity(linreg, X_train, y_train, cv=5, maxPolynomialDegree=5, learning_rate=0.001,\n",
    "                                 epochs=10000, tol=1e-6, regularizer=None, lambd=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad731efa",
   "metadata": {},
   "source": [
    "Hyperparameters we chose are Lambda=0.001, Learning rate=0.001 and Regularizer=L2-Ridge regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b35cecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 Stochastic GD\n",
    "\n",
    "def learningschedule(t):\n",
    "        t0=5\n",
    "        t1=50\n",
    "        return  t0/ (t+t1)\n",
    "    \n",
    "class SGDRegression():\n",
    "    def __init__(self,learningrate=0.01, epochs=100,tol=None,regularizer=None,lambd=0,**kwargs):\n",
    "        self.learningrate=learningrate\n",
    "        self.epochs=epochs\n",
    "        self.tol=tol\n",
    "        self.regularizer=regularizer\n",
    "        self.lambd=lambd\n",
    "        self.kwargs=kwargs\n",
    "        \n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        m,d=X.shape\n",
    "        theta_hat=np.random.randn(d)\n",
    "        newJ=0\n",
    "        if self.regularizer==\"L1\":\n",
    "            for i in range(self.epochs):\n",
    "                for i in range(m):\n",
    "                    random_index = np.random.randint(m)\n",
    "                    xi=X[random_index]\n",
    "                    yi=y[random_index]\n",
    "                    theta_reg=theta_hat.copy()\n",
    "                    theta_reg[0]=0\n",
    "                    self.learningrate=learningschedule(self.epochs*m+i)\n",
    "                    theta_hat=theta_hat-self.learningrate*xi.T.dot(xi.dot(theta_hat)-yi)-((self.learningrate*self.lambd))*(np.sign(theta_reg))\n",
    "                    previousJ=newJ\n",
    "                    newJ=(1/2)*(yi-xi.dot(theta_hat))**2+(self.lambd/2)*np.sum(np.abs(theta_reg))\n",
    "                    if abs(newJ-previousJ) < self.tol:\n",
    "                        break\n",
    "        \n",
    "        elif self.regularizer==\"L2\":\n",
    "            for i in range(self.epochs):\n",
    "                for i in range(m):\n",
    "                    random_index = np.random.randint(m)\n",
    "                    xi=X[random_index]\n",
    "                    yi=y[random_index]\n",
    "                    theta_reg=theta_hat.copy()\n",
    "                    theta_reg[0]=0\n",
    "                    self.learningrate=learningschedule(self.epochs*m+i)\n",
    "                    theta_hat=theta_hat-(self.learningrate)*xi.T.dot(xi.dot(theta_hat)-yi)-((self.learningrate*self.lambd))*theta_reg\n",
    "                    previousJ=newJ\n",
    "                    newJ=(1/2)*(yi-xi.dot(theta_hat))**2+(self.lambd/2)*np.sum(np.square(theta_reg))\n",
    "                    if abs(newJ-previousJ) < self.tol:\n",
    "                        break\n",
    "        \n",
    "        elif self.regularizer==\"none\":\n",
    "            for i in range(self.epochs):\n",
    "                for i in range(m):\n",
    "                    random_index = np.random.randint(m)\n",
    "                    xi=X[random_index]\n",
    "                    yi=y[random_index]\n",
    "                    self.learningrate=learningschedule(self.epochs*m+i)\n",
    "                    theta_hat=theta_hat-(self.learningrate)*xi.T.dot(xi.dot(theta_hat)-yi)\n",
    "                    newJ=(1/2)*(yi-xi.dot(theta_hat))**2\n",
    "                    \n",
    "                    if abs(newJ-previousJ) < self.tol:\n",
    "                        break\n",
    "        self.theta=theta_hat\n",
    "        \n",
    "    def predict(self,X):\n",
    "        Y=X.dot(self.theta)\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e4bedbfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+---------------+----------+\n",
      "|   lamb |   learnrate | regularizer   |      mse |\n",
      "+========+=============+===============+==========+\n",
      "| 1      |      0.1    | L1            | 1.11618  |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 1      |      0.1    | L2            | 0.478741 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 1      |      0.01   | L1            | 1.14473  |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 1      |      0.01   | L2            | 0.482421 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 1      |      0.001  | L1            | 1.13974  |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 1      |      0.001  | L2            | 0.481646 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 1      |      0.0001 | L1            | 1.07     |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 1      |      0.0001 | L2            | 0.48127  |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0      |      0.1    | L1            | 0.483695 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0      |      0.1    | L2            | 0.652064 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0      |      0.01   | L1            | 0.538381 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0      |      0.01   | L2            | 0.537537 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0      |      0.001  | L1            | 0.580288 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0      |      0.001  | L2            | 0.567574 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0      |      0.0001 | L1            | 0.585131 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0      |      0.0001 | L2            | 0.555116 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.1    |      0.1    | L1            | 0.512326 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.1    |      0.1    | L2            | 0.485435 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.1    |      0.01   | L1            | 0.523642 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.1    |      0.01   | L2            | 0.536661 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.1    |      0.001  | L1            | 0.514651 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.1    |      0.001  | L2            | 0.522147 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.1    |      0.0001 | L1            | 0.499293 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.1    |      0.0001 | L2            | 0.516416 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.01   |      0.1    | L1            | 0.557048 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.01   |      0.1    | L2            | 0.48275  |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.01   |      0.01   | L1            | 0.502496 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.01   |      0.01   | L2            | 0.606717 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.01   |      0.001  | L1            | 0.595503 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.01   |      0.001  | L2            | 0.576444 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.01   |      0.0001 | L1            | 0.511054 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.01   |      0.0001 | L2            | 0.52716  |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.001  |      0.1    | L1            | 0.620387 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.001  |      0.1    | L2            | 0.550369 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.001  |      0.01   | L1            | 0.709866 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.001  |      0.01   | L2            | 0.631518 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.001  |      0.001  | L1            | 0.592232 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.001  |      0.001  | L2            | 0.72407  |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.001  |      0.0001 | L1            | 0.667122 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.001  |      0.0001 | L2            | 0.585728 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.0001 |      0.1    | L1            | 0.552771 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.0001 |      0.1    | L2            | 0.568917 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.0001 |      0.01   | L1            | 0.633825 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.0001 |      0.01   | L2            | 0.5215   |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.0001 |      0.001  | L1            | 0.505472 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.0001 |      0.001  | L2            | 0.543916 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.0001 |      0.0001 | L1            | 0.536525 |\n",
      "+--------+-------------+---------------+----------+\n",
      "| 0.0001 |      0.0001 | L2            | 0.513429 |\n",
      "+--------+-------------+---------------+----------+\n",
      "Best Valid MSE is [0.47874110382002977]\n"
     ]
    }
   ],
   "source": [
    "#finding hyperparameters for SGD\n",
    "lamb=[1,0,0.1,0.01,0.001,0.0001]\n",
    "learnrate=[0.1,0.01,0.001,0.0001]\n",
    "regular=[\"L1\",\"L2\"]\n",
    "\n",
    "mylist=[]\n",
    "#index=0\n",
    "for l in lamb:\n",
    "    for lr in learnrate:\n",
    "        for r in regular:\n",
    "            #index+=1\n",
    "            #print(index)\n",
    "            linreg=SGDRegression(learningrate=lr,epochs=500,tol=1e-6,regularizer=r,lambd=l)\n",
    "            pred, true, trainmse, validmse =kFold(5,X_train_poly,y_train,linreg,\"mse\")\n",
    "            mydict={\"lamb\":l,\"learnrate\":lr,\"regularizer\":r,\"mse\":validmse}\n",
    "            mydict_copy=mydict.copy()\n",
    "            mylist.append(mydict_copy)\n",
    "\n",
    "validmselist=list()\n",
    "bestvalidmse=list()\n",
    "for i in range(len(mylist)):\n",
    "    validmselist.append(mylist[i]['mse'])\n",
    "bestvalidmse.append(np.nanmin(validmselist))\n",
    "header=mylist[0].keys()\n",
    "rows= [x.values() for x in mylist]\n",
    "print(tabulate.tabulate(rows,header,tablefmt='grid'))\n",
    "print(\"Best Valid MSE is\", bestvalidmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39542754",
   "metadata": {},
   "source": [
    "Best Hyperparameters with MSE 0.47874110382002977 are Lambda=1.0, Learning rate=0.1 and Regularizer=L2(Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "626326ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9227032329596482, 0.4251462015966422)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SGD on test data\n",
    "\n",
    "#SGD on test data for L1(Lasso)\n",
    "linreg=SGDRegression(learningrate=0.1,epochs=1000,tol=1e-6,regularizer=\"L1\",lambd=1.0)\n",
    "linreg.fit(X_train_poly,y_train)\n",
    "y_predl1=linreg.predict(X_test_poly)\n",
    "sgdmsel1=MSE(y_predl1,y_test)\n",
    "\n",
    "#SGD on test data for L2(Ridge)\n",
    "linreg=SGDRegression(learningrate=0.1,epochs=1000,tol=1e-6,regularizer=\"L2\",lambd=1.0)\n",
    "linreg.fit(X_train_poly,y_train)\n",
    "y_predl2=linreg.predict(X_test_poly)\n",
    "sgdmsel2=MSE(y_predl2,y_test)\n",
    "\n",
    "sgdmsel1,sgdmsel2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f061266",
   "metadata": {},
   "source": [
    "we used lambda=1 with learning rate 0.1 with lambda 1 and regularizer L2 to get a test mse of 0.425"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
